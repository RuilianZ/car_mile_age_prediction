---
title: "Homework 3 for Data Science II"
author: "Roxy Zhang"
date: "3/22/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(caret) # Classification And REgression Training
library(glmnet) # GLM models
library(MASS) # LDA QDA
library(pROC) # ROC curves
library(vip) # variable importance
library(klaR) # visualization
library(pdp) # partial dependence plot
library(AppliedPredictiveModeling) # transparent theme
library(reshape2) # melt() EDA visualization
library(ggcorrplot) # correlation plot

# set theme
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Data import and cleaning

```{r}
auto = read_csv("auto.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low"),
    cylinders = as.factor(cylinders),
    year = as.factor(year),
    origin = case_when(
      origin == "1" ~ "American",
      origin == "2" ~ "European",
      origin == "3" ~ "Japanese"),
    origin = as.factor(origin)
    )

# reorder columns for future visualization
col_order = c("cylinders", "year", "origin",
               "displacement", "horsepower", "weight", "acceleration", "mpg_cat")
auto = auto[ , col_order]

# check for NA
colSums(is.na(auto))
```


## Data partition

Split the dataset into two parts: training data (70%) and test data (30%).

```{r}
set.seed(2570)

index_train = createDataPartition(
  y = auto$mpg_cat,
  p = 0.7,
  list = FALSE
)

train = auto[index_train, ]
test = auto[-index_train, ]

head(train)


# matrix of predictors
# x = model.matrix(mpg_cat~., auto)[ , -1] # remove intercept
# y = test$mpg_cat
```


## Exploratory Data Analysis

Produce some graphical or numerical summaries of the data.

```{r}
dim(auto)

summary(auto)

skimr::skim(auto)
```

There are 392 rows and 8 columns in the full data, including 4 numeric predictors: `displacement`, `horsepower`, `weight`, `acceleration`, 3 categorical predictors: `cylinders`, `year`, `origin`, and 1 categorical response variable: `mpg_cat`.  

```{r}
# visualization for numeric variables using feature plot

# set plot theme
theme1 = transparentTheme(trans = .4)
trellis.par.set(theme1)

# density plot
featurePlot(
  x = auto %>% dplyr::select(displacement, horsepower, weight,  acceleration), 
  y = auto$mpg_cat, 
  scales = list(x = list(relation = "free"),
                y = list(relation = "free")),
  plot = "density",
  pch = "|",
  auto.key = list(columns = 2))
```

The feature plot shows that higher MPG category is associated with lower weight,  higher acceleration, lower displacement and lower horsepower.

```{r}
# visualization for categorical variables using ggplot

auto %>% 
  dplyr::select(-displacement, -horsepower, -weight, -acceleration) %>% 
  melt(id.vars = "mpg_cat") %>% 
  ggplot(aes(x = value, fill = mpg_cat)) + 
  geom_bar(position = "fill") + 
  #scale_y_continuous(labels = scales::percent) + # % on y axis
  labs(x = "",
       y = "Proportion",
       fill = "MPG Category", # legend title
       color = "MPG Category") +
  facet_wrap(~variable, scales = "free", nrow = 2)
```

This plot shows that higher MPG category mainly lies in cars with 5 or 6 cylinders, model year 1908s, and origin of European and Japanese.  

```{r}
# LAD partition plot for numeric variables
partimat(
  mpg_cat ~ displacement + horsepower + weight + acceleration,
  data = auto,
  # subset = index_train, # if want to use test data
  method = "lda")
```

The LDA partition plot is based on every combination of two numeric variables, which gives the decision boundrary of making classification.  
Red labels are misclassified data.  
Although in LDA we use all the predictors rather than just the combination of two predictors, this plot shows some potential patterns of the data (since we cannot visualize things easily in high-dimensional space).  

```{r}
# correlation plot for all data
model.matrix(mpg_cat~., data = auto)[ , -1] %>% 
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot(type = "full", lab = TRUE, lab_size = 1)
```

We can see from the correlation plot that the numeric predictors `displacement`, `horsepower`, `weight`, `acceleration` are highly correlated, which may potentially result in some redundancy for model building.  
Also, `cylinders8` is highly correlated with above numeric predictors.
