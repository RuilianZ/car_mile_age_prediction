---
title: "Homework 3 for Data Science II"
author: "Roxy Zhang"
date: "3/22/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret) # Classification And REgression Training
library(glmnet) # GLM models
library(MASS) # LDA QDA
library(pROC) # ROC curves
library(vip) # variable importance
library(klaR) # visualization
library(pdp) # partial dependence plot
library(AppliedPredictiveModeling) # transparent theme
library(reshape2) # melt() EDA visualization
library(ggcorrplot) # correlation plot

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE, 
  fig.align = "center",
  fig.width = 6,
  fig.asp = .7,
  out.width = "95%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Data import and cleaning

```{r}
auto = read_csv("auto.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low"),
    cylinders = as.factor(cylinders),
    year = as.factor(year),
    origin = case_when(
      origin == "1" ~ "American",
      origin == "2" ~ "European",
      origin == "3" ~ "Japanese"),
    origin = as.factor(origin)
    )

# reorder columns for future visualization
col_order = c("cylinders", "year", "origin",
               "displacement", "horsepower", "weight", "acceleration", "mpg_cat")
auto = auto[ , col_order]

# check for NA
colSums(is.na(auto))
```


## Data partition

Split the dataset into two parts: training data (70%) and test data (30%).

```{r}
set.seed(2570)

index_train = createDataPartition(
  y = auto$mpg_cat,
  p = 0.7,
  list = FALSE
)

train = auto[index_train, ]
test = auto[-index_train, ]

head(train)


# matrix of predictors
# x = model.matrix(mpg_cat~., auto)[ , -1] # remove intercept
# y = test$mpg_cat
```


## Exploratory Data Analysis

Produce some graphical or numerical summaries of the data.  

```{r}
dim(auto)

summary(auto)

skimr::skim(auto)
```

There are 392 rows and 8 columns in the full data, including 4 numeric predictors: `displacement`, `horsepower`, `weight`, `acceleration`, 3 categorical predictors: `cylinders`, `year`, `origin`, and 1 categorical response variable: `mpg_cat`.  

For better illustration, all EDA plots are done using train data.

```{r}
# visualization for numeric variables using feature plot

# set plot theme
theme1 = transparentTheme(trans = .4)
trellis.par.set(theme1)

# density plot
featurePlot(
  x = train %>% dplyr::select(displacement, horsepower, weight,  acceleration), 
  y = train$mpg_cat, 
  scales = list(x = list(relation = "free"),
                y = list(relation = "free")),
  plot = "density",
  pch = "|",
  auto.key = list(columns = 2))
```

The feature plot shows that higher MPG category is associated with lower weight,  higher acceleration, lower displacement and lower horsepower.

```{r}
# visualization for categorical variables using ggplot

train %>% 
  dplyr::select(-displacement, -horsepower, -weight, -acceleration) %>% 
  melt(id.vars = "mpg_cat") %>% 
  ggplot(aes(x = value, fill = mpg_cat)) + 
  geom_bar(position = "fill") + 
  #scale_y_continuous(labels = scales::percent) + # % on y axis
  labs(x = "",
       y = "Proportion",
       fill = "MPG Category", # legend title
       color = "MPG Category") +
  facet_wrap(~variable, scales = "free", nrow = 2)
```

This plot shows that higher MPG category mainly lies in cars with 5 or 6 cylinders, model year 1908s, and origin of European and Japanese.  

```{r}
# LDA partition plot for numeric variables
partimat(
  mpg_cat ~ displacement + horsepower + weight + acceleration,
  data = auto,
  subset = index_train,
  method = "lda")
```

The LDA partition plot is based on every combination of two numeric variables, which gives the decision boundrary of making classification.  
Red labels are misclassified data.  
Although in LDA we use all the predictors rather than just the combination of two predictors, this plot shows some potential patterns of the data (since we cannot visualize things easily in high-dimensional space).  

```{r}
# correlation plot for all data
model.matrix(mpg_cat~., data = train)[ , -1] %>% 
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot(type = "full", lab = TRUE, lab_size = 1)
```

We can see from the correlation plot that the numeric predictors `displacement`, `horsepower`, `weight`, `acceleration` are highly correlated, which may potentially result in some redundancy for model building.  
Also, `cylinders8` is highly correlated with above numeric predictors.


## Logistic Regression

```{r}
set.seed(1115)

# check for the response variable level
contrasts(auto$mpg_cat)

# fit glm model
glm_fit = glm(
  mpg_cat ~ .,
  data = auto,
  subset = index_train,
  family = binomial(link = "logit"))

summary(glm_fit)
```

From the summary above, we can see that for the logistic regression model, `weight` and `originEuropean` are **statistically significant predictors** under 0.05 significance level, and `weight` is significant under 0.01 significance level.  

```{r}
# test model performance
test_pred_prob = predict(
  glm_fit,
  newdata = test,
  type = "response") # get predicted probabilities

test_pred = rep("low", length(test_pred_prob))

# use a simple classifier with a cut-off of 0.5
test_pred[test_pred_prob>0.5] = "high"

confusionMatrix(data = as.factor(test_pred),
                reference = test$mpg_cat,
                positive = "high")
```

* As the confusion matrix shows above, there are 52 true low MPG category and 50 true high MPG category, with a prediction accuracy of 0.8793.  
* The No Information Rate is 0.5, which means if we have no information at all and predict all the MPG category to be low (or high), the prediction accuracy will be 0.5.  
* The p-value is approximately 0, showing that the fitted model is significantly better than the one generates no information rate.  
* Sensitivity is 0.8621, which is the rate of predicting MPG category as high given the true value is high. Specificity is 0.8966, which is the rate of predicting MPG category as low given the true value is low.  
* Positive predictive value is 0.8929, which is the rate of a true high value given the predicted value is high. Negative predictive value is 0.8929, which is the rate of a true low value given the predicted value is low.  
* Kappa is 0.7586, which means the agreement of observations and predictions is relatively high.


## MARS (multivariate adaptive regression spline) model

```{r}
set.seed(1115)

ctrl = trainControl(
  method = "repeatedcv",
  summaryFunction = twoClassSummary,
  repeats = 5,
  classProbs = TRUE)

mars_fit = train(
  x = train[ , 1:7],
  y = train$mpg_cat,
  method = "earth",
  tuneGrid = expand.grid(degree = 1:4,
                         nprune = 2:18),
  metric = "ROC",
  trControl = ctrl)

summary(mars_fit)

plot(mars_fit)

mars_fit$bestTune

coef(mars_fit$finalModel)

# importance plot
vip(mars_fit$finalModel)
```

* From 'earth', the best tune metrics are nprune = 6, degree = 1, which is consistent with the ROC curve in the plot.  
* There are 6 terms in the final model: intercept, `cylinders4`, `year73`, `h(displacement-163)`, `h(displacement-200)`,  `h(displacement-183)`.  
* From the importance plot above, there are 4 important predictors: `year73`, `cylinders4`, `displacement`.


## LDA

```{r}
lda_fit = lda(
  mpg_cat~.,
  data = auto,
  subset = index_train)

lda_pred = predict(lda_fit, newdata = auto[-index_train, ])

# probabilities of reponse level
head(lda_pred$posterior)

# plot linear discriminants
plot(lda_fit, col = as.numeric(auto$mpg_cat), abbrev = TRUE)

# scaling matrix
lda_fit$scaling
```

* LDA has no tuning parameters, it classifies the data by nearest centroid. Since there are 2 levels of the response variable, we have k = 2 - 1 = 1 linear discriminants.  
* The linear discriminant plot shows the histogram of  transformed X (predictors) for both levels. From the plot, when X is lower, data are tend to be classified in the high `mpg_cat` group, and vice versa.  


## Model comparison

### ROC and AUC

```{r}
glm_pred = predict(
  glm_fit, 
  newdata = auto[-index_train, ], 
  type = "response")

mars_pred = predict(
  mars_fit, 
  newdata = auto[-index_train, ], 
  type = "prob")[,2]

lda_pred = predict(
  lda_fit, 
  newdata = auto[-index_train,])$posterior[,2]

# ROC
glm_roc = roc(auto$mpg_cat[-index_train], glm_pred)
mars_roc = roc(auto$mpg_cat[-index_train], mars_pred)
lda_roc = roc(auto$mpg_cat[-index_train], lda_pred)

# AUC
auc = c(glm_roc$auc[1], mars_roc$auc[1], lda_roc$auc[1])

model_names = c("glm","mars","lda")

# plot ROC curve
dev.off() # to fix "invalid graphics state" error in ggroc

ggroc(
  list(glm_roc, mars_roc, lda_roc), 
  legacy.axes = TRUE) + 
  scale_color_discrete(
    labels = paste0(
      model_names, 
      " (", round(auc, 3),")"),
    name = "Models (AUC)") +
  geom_abline(intercept = 0, slope = 1, color = "grey")
```

The LDA model or logistic regression model is preferred, since they have higher AUC than MARS model.  

```{r}
set.seed(1115)

# refit glm and lda models using caret to incorporate cross-validation
glm_caret = 
  train(mpg_cat~.,
        data = auto,
        method = "glm",
        metric = "ROC",
        trControl = ctrl)

lda_caret = 
  train(mpg_cat~.,
        data = auto,
        method = "lda",
        metric = "ROC",
        trControl = ctrl)

res = resamples(
  list(Logistic_Regression = glm_caret,
       MARS = mars_fit,
       LDA = lda_caret),
  times = 100)

summary(res)

# plot ROC
bwplot(res, metric = "ROC")
```

From the plot above, with cross-validation, the LDA model is preferred since it has the highest ROC.


### Misclassfication error rate

```{r}
# function of calculating misclassfication error rate
error_rate = function(model_name, pred_prob, cutoff){
  pred.label = rep("low", length(pred_prob))
  pred.label[pred_prob > cutoff] = "high"
  confusionMatrix = 
       table(
       tibble(pred = pred.label,
              reference = auto$mpg_cat[-index_train]))
  error = (confusionMatrix['high','low'] + confusionMatrix['low','high'])/length(pred_prob)
  print(error)
}
```

```{r}
# when cut-off is 0.5
error_rate('Logistic Regression', glm_pred, 0.5)
error_rate('MARS', mars_pred, 0.5)
error_rate('LDA', lda_pred, 0.5)
```

* Using a simple classifier with a cut-off of 0.5, Logistic Regression model is the best, since it has the lowest misclassification rate.  
* One can also use the function to explore other situation with different cut-off or models.  
* The higher the cut-off is, the more data will be classified to "lower" class.
